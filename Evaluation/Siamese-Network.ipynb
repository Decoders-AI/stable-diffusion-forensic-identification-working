{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torchvision.datasets as dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import PIL.ImageOps\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigClass():\n",
    "\n",
    "    train_dir = './data/faces/training'\n",
    "    test_dir = './data/faces/testing'\n",
    "    \n",
    "    train_batch_size = 64\n",
    "    train_num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siamese Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_folder, transform = None, invert = False):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.invert = invert\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample_1 = random.choice(self.image_folder.imgs)\n",
    "        same_class = random.randint(0, 1)\n",
    "\n",
    "        if same_class:\n",
    "            while True:\n",
    "                sample_2 = random.choice(self.image_folder.imgs)\n",
    "                if sample_1[1] == sample_2[1]:\n",
    "                    break\n",
    "        else:\n",
    "            while True:\n",
    "                sample_2 = random.choice(self.image_folder.imgs)\n",
    "                if sample_1[1] != sample_2[1]:\n",
    "                    break\n",
    "\n",
    "        img_1 = Image.open(sample_1[0]).convert(\"L\")\n",
    "        img_2 = Image.open(sample_2[0]).convert(\"L\")\n",
    "\n",
    "        if self.invert:\n",
    "            img_1 = PIL.ImageOps.invert(img_1)\n",
    "            img_2 = PIL.ImageOps.invert(img_2)\n",
    "\n",
    "        if self.transform:\n",
    "            img_1 = self.transform(img_1)\n",
    "            img_2 = self.transform(img_2)\n",
    "\n",
    "        return img_1, img_2, torch.from_numpy(np.array([int(sample_1[1] != sample_2[1])], dtype = np.float32))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_folder.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, weights_path=\"Model_Finished.pth\"):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "\n",
    "        # Load the weights from the saved model\n",
    "        if weights_path is not None:\n",
    "            pretrained_dict = torch.load(weights_path)\n",
    "            model_dict = self.state_dict()\n",
    "            pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "            model_dict.update(pretrained_dict)\n",
    "            self.load_state_dict(model_dict)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        \n",
    "        output1 = self.layer1(img1)\n",
    "        output2 = self.layer2(img2)\n",
    "        output = torch.cat((output1, output2), dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image_folder = dataset.ImageFolder(root=ConfigClass.train_dir)\n",
    "siamese_dataset = SiameseDataset(image_folder=image_folder,\n",
    "                                 transform=transforms.Compose([transforms.Resize((100, 100)), transforms.ToTensor()]),\n",
    "                                 invert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, text=None):\n",
    "    np_img = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic', fontweight='bold', bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def show_plot(iteration,loss):\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(siamese_dataset, shuffle=True, num_workers=8, batch_size=8)\n",
    "data_iter = iter(dataloader)\n",
    "vis_batch = next(data_iter)\n",
    "merged = torch.cat((vis_batch[0], vis_batch[1]), 0)\n",
    "imshow(torchvision.utils.make_grid(merged))\n",
    "vis_batch[2].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "\n",
    "        self.reflection_pad = nn.ReflectionPad2d(1)\n",
    "        self.conv1 = nn.Conv2d(1, 4, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(4, 8, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(8, 8, kernel_size=3)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(4)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(8)\n",
    "        self.fc1 = nn.Linear(8 * 100 * 100, 500)\n",
    "        self.fc2 = nn.Linear(500, 500)\n",
    "        self.fc3 = nn.Linear(500, 5)\n",
    "\n",
    "    def forward_one_branch(self, x):\n",
    "        x = self.batch_norm1(self.relu(self.conv1(self.reflection_pad(x))))\n",
    "        x = self.batch_norm2(self.relu(self.conv2(self.reflection_pad(x))))\n",
    "        x = self.batch_norm2(self.relu(self.conv3(self.reflection_pad(x))))\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one_branch(input1)\n",
    "        output2 = self.forward_one_branch(input2)\n",
    "\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstrastiveLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ConstrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        distance = F.pairwise_distance(output1, output2, keepdim=True)\n",
    "        contrastive_loss = torch.mean((1 - label)*torch.pow(distance, 2)\n",
    "                                      + (label)*torch.pow(torch.clamp(self.margin - distance, min=0.0), 2))\n",
    "\n",
    "        return contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(siamese_dataset, shuffle=True, num_workers=8, batch_size=ConfigClass.train_batch_size)\n",
    "model = SiameseNetwork()\n",
    "criterion = ConstrastiveLoss()  # Replace this with your own implementation or a compatible alternative\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "counter = []\n",
    "loss_history = []\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traning Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader = DataLoader(siamese_dataset, shuffle=True, num_workers=0, batch_size=ConfigClass.train_batch_size // 2)\n",
    "\n",
    "for epoch in range(ConfigClass.train_num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        input1, input2, label = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move the tensors to the desired device (CPU or CUDA) inside the DataLoader loop\n",
    "        input1, input2, label = input1.to(torch.device('cpu')), input2.to(torch.device('cpu')), label.to(torch.device('cpu'))\n",
    "\n",
    "        output1, output2 = model(input1, input2)\n",
    "        contrastive_loss = criterion(output1, output2, label)\n",
    "        contrastive_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(\"Epoch: {} \\t Loss: {}\".format(epoch, contrastive_loss.item()))\n",
    "            iteration += 10\n",
    "            loss_history.append(contrastive_loss.item())\n",
    "            counter.append(iteration)\n",
    "\n",
    "\n",
    "show_plot(counter, loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = dataset.ImageFolder(root=ConfigClass.test_dir)\n",
    "siamese_dataset = SiameseDataset(image_folder=test_folder,\n",
    "                                 transform=transforms.Compose([transforms.Resize((100, 100)), transforms.ToTensor()]),\n",
    "                                 invert=False)\n",
    "\n",
    "dataloader = DataLoader(siamese_dataset, num_workers=6, batch_size=1, shuffle=True)\n",
    "data_iter = iter(dataloader)\n",
    "img0, _, _ = next(data_iter)\n",
    "\n",
    "for i in range(10):\n",
    "    _, img1, label = next(data_iter)\n",
    "    merged = torch.cat((img0,img1), 0)\n",
    "\n",
    "    output1, output2 = model(img0, img1)\n",
    "    distance = F.pairwise_distance(output1, output2)\n",
    "    imshow(torchvision.utils.make_grid(merged), 'Dissimilarity: {:.2f}'.format(distance.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_0 = \"/content/2a.png\"\n",
    "image_path_1 = \"/content/gen2b.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "       \n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        output1 = self.layer1(img1)\n",
    "        output2 = self.layer2(img2)\n",
    "        output = torch.cat((output1, output2), dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torchvision.utils as utils\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.Grayscale(),  # Convert to grayscale\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load and transform images\n",
    "img0 = transform(Image.open(image_path_0).convert(\"RGB\"))\n",
    "img1 = transform(Image.open(image_path_1).convert(\"RGB\"))\n",
    "\n",
    "# Pass images through the model\n",
    "output1, output2 = model(img0.unsqueeze(0), img1.unsqueeze(0))\n",
    "distance = F.pairwise_distance(output1, output2)\n",
    "\n",
    "# Show images and distance\n",
    "merged = torch.cat((img0, img1), 1)  # Concatenate along channel dimension\n",
    "\n",
    "grid = utils.make_grid(merged)\n",
    "\n",
    "\n",
    "imshow(grid, 'Dissimilarity: {:.2f}'.format(distance.item()))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
